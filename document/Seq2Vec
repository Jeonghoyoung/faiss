Seq2Vec

Sequence data 
	- 시간성 : 특징을 순서대로 한번에 하나씩 입력해야 한다.
	- 가변 길이 : 길이가 t인 샘플을 처리하려면 은닉층이 t번 나타나야 한다. (t는 가변적)
	- 문맥 의존성 : 이전 특징 내용을 기억하고 있다가 적절한 순간에 활용해야 한다.
	* 이러한 기능을 모두 갖춘 학습모델을 MLP(Multi-Layer Perceptron)를 확장하여 설계할 수 있다.

Sequential data(순차데이터) : 데이터 집합내의 객체들이 어떤 순서를 가진 데이터로 그 순서가 변경될 경우 고유의 특성을 잃어버리는 특징이 있다.


One hot encoding : 전통적인 NLP연구에서 주로 사용하던 방법으로 메모리에 있어서 비효율적이란 점과 학습중에 의미있는 관계표현을 무시할 수 있다는 단점이 있다.(각 단어 벡터간 유의미한 유사성 표현 불가)

Word embedding : 단어의 의미를 다차원 공간에 벡터화 하는 방법인 분산 표현을 이용하여 단어 간 의미적 유사성을 벡터화 하는 작업
	* 분산 표현 : 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다는 가정하에 만들어진 표현 방법. 
				예를 들어, 강아지란 단어는 귀엽다, 예쁘다 등의 단어가 함께 등장한다고 하면, 분포 가설에 따라서 해당 내용을 가진 텍스트의 단어들을 벡터화한다면 해당 단어 벡터들은 유사한 벡터값을 가진다. 
				** 분산 표현은 분포 가설을 이용하여 텍스트를 학습하고 단어의 의미를 벡터의 여러차원에 분산하여 표현한다.
				** 저차원에 단어의 의미를 여러차원에다 분산하여 표현, 이런 방법을 사용하면 단어 벡터간 유의미한 유사도를 계산할 수 있다.

Word2Vec으로 임베딩 된 벡터는 벡터 차원이 단어 집합의 그 키가 될 필요가 없으므로 사용자가 설정한 차원의 수를 가지는 벡터가 되며 각 차원의 값은 실수값을 가진다.

- Word2Vec 학습 방식 -
	1. CBOW (Continuous Bag Of Words) : 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법.
		- 예를들어 "The fat cat sat on the mat" 라는 예문에서 갖고있는 코퍼스가 ['The', 'fat', 'cat', 'on', 'the', 'mat'] 라고 할때, 코퍼스로 부터 'sat'를 예측하는 것은 CBOW가 하는 일이며, 이때 예측해야하는 단어 'sat'를 중심단어 (center word)라고 하고, 예측에 사용되는 단어들을 주변 단어(context word)라고 한다. 
		- 중심 단어를 예측하기 위해 앞, 뒤로 몇개의 단어를 볼지를 결정해야 하는데 이 범위를 윈도우(window)라 한다.
		- 윈도우 = 2인 경우 중심단어 'sat'의 앞의 두단어 'fat', 'cat' 그리고 뒤의 단어인 'on', 'the'를 입력으로 사용한다.
		- 슬라이딩 윈도우(sliding window) : 윈도우의 크기가 정해지면 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해 가며 학습을 위한 데이터 셋을 만드는 방법.
		- 입력층(Input layer)의 입력으로서 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 원-핫 벡터가 들어가게 되고 출력층(Output layer)에서 예측하고자 하는 중간 단어의 원-핫 벡터가 레이블로서 필요 하다.
		- Word2Vec의 은닉층은 일반적인 은닉층과는 달리 활성화 함수가 아닌 룩업 테이블이라는 연산을 담당하는 층으로 투사층이라고 부르기도 한다.
			- 룩업테이블 : i번째 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 입력 벡터와 가중치 W 행렬의 곱은 W행렬의 i번째 행을 그대로 읽어 오는것.
				- lookup해온 W의 각 행벡터가 Word2Vec학습 후에는 각 단어의 M차원의 임베딩 벡터로 입력층과 투사층 사이의 가중치 와 투사층에서 출력층 사이의 가중치를 잘 훈련시키는것이 CBOW의 목적이다.
		- CBOW는 소프트맥스 함수를 지나면서 벡터의 각 원소들의 값은 0과 1사이의 실수로 총합은 1이다.

	2. Skip-Gram : 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법.
		- 중심단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없으며 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다.

seq2vec : MRE(다중 순위 임베딩)을 사용하여 임의의 순차 데이터를 rank-2형식으로 나타내는 방식.
	- 다중 순위를 사용하여 시퀀스의 시간정보를 유지하는 동시에 데이터의 중요한 특성을 작업하기 쉬운 수치 벡터에 포함한다.
		- 행은 벡터의 시간적 순서를 의미하고 각 행은 데이터 블럭의 임베딩 벡터를 나타낸다.
	- Sec2Vec는 효율적인 Sequential data (순차 데이터) 분석을 위한 다음 조건을 만족한다.
		1. Input data의 사이즈를 줄인다.
			- 최적의 벡터 크기는 입력 데이터의 복잡성에 따라 다르므로 사이즈를 줄이는 것은 메모리 사용량을 줄이고 편리한 실시간 분석을 가능케한다.
		2. 원시 순차 데이터에 포함된 시변 정보를 유지한다.
			- seq2vec를 사용한 후속 분석에서 이 시퀀스 정보는 CNN 또는 LSTM에서 시변 문맥을 추출하는데 사용할 수 있다.
		3. seq2vec는 모든 범주형 데이터를 숫자 데이터와 함꼐 인코딩 할 수 있다.
			- 범주형 또는 연속적인 추가 feature를 feature 벡터로 추가할 수 있다.
		4. seq2vec 벡터는 벡터 산술을 사용하여 수학적으로 서로 비교할 수 있고 딥 러닝 모델에 대한 입력으로 사용할 수 있는 의미 있는 벡터 구성 요소를 나타낼 수 있다.
			- 임베딩 기술을 사용하는 주요 장점이다.
		5. seq2vec은 원시 데이터에 포함된 민감한 개인정보를 난독화 하는 동시에 데이터 세트에 대부분의 유용한 정보를 유지함으로써 개인 정보 보호 문제를 크게 줄이면서 공유 및 협업을 가능하게 한다.
		6. seq2vec의 output은 딥러닝 모델에 대한입력으로 직접 사용할 수 있으며, 추가적인 전처리가 필요하지 않거나 거의 없다.

	- Seq2Vec 모델 개발은 두가지 단계로 나누어진다.
		1. 블럭 정의.
			- Input 데이터는 지정된 시간 또는 이벤트 범위에 따라 블록으로 분할되어야 한다.
				- 만약 시간 범위의 1일 데이터인 경우 24 samples, 6일의 경우 (24, 6), 측정 기간을 1주일로 변경하고 임베딩 벡터 크기가 10인 경우 (168, 10)이 된다.

		2. 각 블럭의 임베딩 벡터 생성.
			- 블록에서 임베딩 벡터를 얻으려면 임베딩 모델을 선택해야 한다.
				- 임베딩 벡터 생성 방법
					1. 딥러닝 Auto 인코더 사용.
						- 인코더 모델을 사용하여 원본 정보를 작은 숫자 벡터로 압축 -> 디코더 모델을 사용하여 작은 숫자 벡터를 다시 원본 데이터와 유사한 구조로 해제한다.
						- 일반적으로 원시데이터의 주요 특성을 더 작은 숫자로 압축하는데 사용된다.


